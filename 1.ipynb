{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff46511",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ğŸ“Œ 1. Install Dependencies\n",
    "!pip install transformers datasets pillow torch torchvision tqdm -q\n",
    "\n",
    "# ğŸ“Œ 2. Imports\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n",
    "# ğŸ“Œ 3. Download Flickr8k\n",
    "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
    "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
    "!unzip -q Flickr8k_Dataset.zip\n",
    "!unzip -q Flickr8k_text.zip\n",
    "\n",
    "# ğŸ“Œ 4. Load Captions\n",
    "captions = []\n",
    "with open('Flickr8k_text/Flickr8k.token.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        img_caps = line.strip().split('\\t')\n",
    "        img_id, cap = img_caps[0].split('#')[0], img_caps[1]\n",
    "        captions.append((os.path.join('Flicker8k_Dataset', img_id), cap))\n",
    "\n",
    "df = pd.DataFrame(captions, columns=['image_path', 'caption'])\n",
    "df = df.sample(3000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# ğŸ“Œ 5. Stylize Captions with NLP flair\n",
    "EMOJIS = [\"âœ¨\", \"ğŸŒŸ\", \"ğŸ’«\", \"ğŸ”¥\", \"ğŸ’–\", \"ğŸ“¸\", \"ğŸŒˆ\", \"ğŸŒ¸\", \"ğŸ˜\", \"ğŸ§¡\"]\n",
    "HASHES = [\"#InstaVibes\", \"#Aesthetic\", \"#Mood\", \"#PhotoDump\", \n",
    "          \"#Wanderlust\", \"#ChillScene\", \"#DailyInspo\"]\n",
    "\n",
    "def add_style(c):\n",
    "    words = c.split()\n",
    "    # Keep first few words, maybe shuffle for flair\n",
    "    snippet = \" \".join(random.sample(words, min(len(words), 6)))\n",
    "    snippet = snippet.capitalize()\n",
    "    return f\"{snippet} {random.choice(EMOJIS)} {random.choice(HASHES)}\"\n",
    "\n",
    "df['caption'] = df['caption'].apply(add_style)\n",
    "df.to_csv('stylized_captions.csv', index=False)\n",
    "print(\"Stylized captions head:\\n\", df.head())\n",
    "\n",
    "# ğŸ“Œ 6. Dataset Class\n",
    "class InstaDataset(Dataset):\n",
    "    def __init__(self, df, processor):\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.df.iloc[idx]['image_path']\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        caption = self.df.iloc[idx]['caption']\n",
    "        inputs = self.processor(images=image, text=caption, \n",
    "                                 padding=\"max_length\", truncation=True, max_length=128,\n",
    "                                 return_tensors=\"pt\")\n",
    "        inputs['labels'] = inputs['input_ids']\n",
    "        return {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "\n",
    "# ğŸ“Œ 7. Load Model & Processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# ğŸ“Œ 8. Prepare DataLoader\n",
    "df2 = pd.read_csv('stylized_captions.csv')\n",
    "dataset = InstaDataset(df2, processor)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([b['input_ids']     for b in batch]),\n",
    "        'attention_mask': torch.stack([b['attention_mask'] for b in batch]),\n",
    "        'pixel_values': torch.stack([b['pixel_values']  for b in batch]),\n",
    "        'labels': torch.stack([b['labels']           for b in batch]),\n",
    "    }\n",
    "\n",
    "# ğŸ“Œ 9. Training Setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./blip_fancy\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=collate_fn\n",
    ")\n",
    "\n",
    "# ğŸ“Œ 10. Start Training\n",
    "trainer.train()\n",
    "\n",
    "# ğŸ“Œ 11. Save Model & Processor\n",
    "model.save_pretrained(\"blip_fancy_model\")\n",
    "processor.save_pretrained(\"blip_fancy_model\")\n",
    "\n",
    "# ğŸ“Œ 12. Inference Test on New Image\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "def make_fancy(c):\n",
    "    EMOJIS = [\"âœ¨\", \"ğŸŒŸ\", \"ğŸ’«\", \"ğŸ”¥\", \"ğŸ’–\"]\n",
    "    STARTERS = [\"Serving looks ğŸ˜ â€”\", \"Just vibes âœ¨\", \"Golden hour glory ğŸŒ…\"]\n",
    "    ENDERS = [\"#mood\", \"#wanderlust\", \"#aesthetic\"]\n",
    "    cap = c[0].upper() + c[1:]\n",
    "    return f\"{random.choice(STARTERS)} {cap} {random.choice(EMOJIS)} {random.choice(ENDERS)}\"\n",
    "\n",
    "test_img = Image.open(\"your_test_image.jpg\").convert('RGB')\n",
    "inp = processor(images=test_img, return_tensors=\"pt\").to(model.device)\n",
    "out = model.generate(**inp)\n",
    "plain = processor.decode(out[0], skip_special_tokens=True)\n",
    " fancy = make_fancy(plain)\n",
    "print(\"Plain:\", plain)\n",
    "print(\"Fancy:\", fancy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b726a2a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# install required packages\n",
    "# pip install transformers datasets torch torchvision accelerate\n",
    "\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Load BLIP processor and model\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Use GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Custom dataset: Assuming CSV with columns ['image_path', 'caption']\n",
    "def load_custom_dataset(csv_path):\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    dataset_dict = {\n",
    "        \"image\": [Image.open(p).convert(\"RGB\") for p in df[\"image_path\"]],\n",
    "        \"caption\": df[\"caption\"].tolist()\n",
    "    }\n",
    "    return Dataset.from_dict(dataset_dict)\n",
    "\n",
    "# Preprocess dataset\n",
    "def preprocess_function(examples):\n",
    "    inputs = processor(images=examples[\"image\"], text=examples[\"caption\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    return {\n",
    "        \"pixel_values\": inputs[\"pixel_values\"][0],\n",
    "        \"input_ids\": inputs[\"input_ids\"][0],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"][0],\n",
    "        \"labels\": inputs[\"input_ids\"][0]\n",
    "    }\n",
    "\n",
    "# Load and preprocess dataset\n",
    "dataset = load_custom_dataset(\"fancy_captions.csv\")\n",
    "processed_dataset = dataset.map(preprocess_function)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./blip-finetuned-instagram\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset,\n",
    "    tokenizer=processor,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save fine-tuned model\n",
    "model.save_pretrained(\"./blip-finetuned-instagram\")\n",
    "processor.save_pretrained(\"./blip-finetuned-instagram\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c39d94e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03abb3c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
